\documentclass[a4paper,12pt]{report}
\usepackage{amsmath} %Use align

%Temporary stuff
\usepackage{draftcopy}
\usepackage[footnote,draft,silent,nomargin]{fixme}

\begin{document}

\title{Models in Pixel Based Phenology}
\author{Joel Granados \\ jogr@itu.dk}
\date{ \today }

\maketitle

\section*{Introduction}
Phenology is the study of plant and animal cycles and how they relate to the
seasons. Phenological measurements are of great importance to ecologists for a
number of reasons. We are interested not in the direct consequence of
phenological measurements, but in the way these measurements are done. More
specifically, we are studying a way to use digital imaging to automate plant
phenology data gathering.

\section{Description of the Data}
The data that we are handling in this experiment comes from a pan tilt zoom
camera. This camera takes pictures of a region of interest. This is done
throughout various seasons on a daily basis (Sometimes even more frequently).
Not all the pictures are usable, some of them just contain dirt and
uninteresting elements. Some, however, contain plants that interest the
biologist. It is these images are the initial data points. Before the data
analysis a coherent series of images (that describe a season) needs to be
created.

After having 1 or several season image series, we can begin the pixel color
analysis. Our next step is to annotate images from one season\footnote{We
annotate images from one season in the hope that the sample will be
representative of all seasons; and can be used to detect posterior seasons.}.
Annotations consist in identifying pixels of interest.
We create two types of annotations: background annotations and foreground
annotations. Foreground is defined as whatever is of interest to
the biologist. This could be a flower, fern, a whole tree or a part of a tree.
Background is defined as the rest of the image. Notice that in our model the
foreground annotations are not necessarily the compliment of the background
ones.

After the annotations we get two pixel lists: a background pixel list and a
foreground pixel list. These two lists will be the base of our Naive Bayesian
Models. We will denominate the number of foreground pixels as $N_{FG}$ and
likewise denominate the number of background pixels as $N_{BG}$. Using probability
theory \fxnote{cite Pattern Recognition and Machine Learning} we can calculate
the probability of a pixel being foreground or background\footnote{We will
denote the random variable expression foreground or background and $\Phi$}
by using the following equations:

\begin{equation}
P(\Phi= bg) = P(bg) = \frac{N_{BG}}{N_{BG}+N_{FG}}
\end{equation}

\begin{equation}
P(\Phi= fg) = P(fg) = \frac{N_{FG}}{N_{FG}+N_{BG}}
\end{equation}

For these probabilities to have any kind of meaning $N_{BG}$ and $N_{FG}$ need
to be calculated throughout a season. Note that if this is not possible we will
replace the values with something "sensible" like $P(fg)=.5$ and $P(bg)=.5$.
Further notice that we will use the notation from
\fxnote{cite pattern recognition and machine learning page 14} where we will use
lower case characters ($bg$) to denote the value of a random variable ($\Phi$).
The random variable is denoted by an upper case character.

Depending on the color space each pixel contains several sub-dimensions. If we
were to choose the three values of the RGB colors space, the pixel would have 3
sub-dimensions. There are lots of colors spaces and we have lots of
sub-dimensions to choose from. For the purpose of this document we assume that
each pixel has one or more sub-dimensions $D={D_1, D_2... D_n}$

\section{Discrete Model}
The first step is to discretize the values of each dimension. Technically the
values are already discrete. Like the 8 bit value of one of the dimensions of
the RGB color space that has 256 discrete values. But the model
can define a different number of bins. This value is a parameter for the
whole Discrete model.

We define the Naive Bayesian Model for any amount of sub-dimensions with the
following equation:

\begin{align}
P(\Phi \mid D)
    &= \frac{P(D \mid \Phi) P(\Phi)}
            {P(D)} \\
    &= \frac{P(D_1, D_2...D_n \mid \Phi) P(\Phi)}
            {P(D)} \\
    &= \frac{P(D_1 \mid \Phi)
             P(D_2 \mid \Phi) ...
             P(D_n \mid \Phi) P(\Phi)}
            {P(D)} \\
    &= \frac{\prod_{i=1}^n{P(D_i \mid \Phi)} P(\Phi)}
            {P(D)} \\
\end{align}

Therefore given a specific value $d={d_1,d_2...d_n}$ in the n-dimensional
pixel space, the probability for that specific pixel to be foreground is
given by:

\begin{equation}
P(fg \mid d) = \frac{\prod_{i=1}^n{P(d_i \mid fg)} P(fg)}{P(d)}
\end{equation}

We define $P(D_i \mid \Phi)$ as the probability of a pixel having the value $D_i$
given that it has been cataloged as $\Phi$. This conditional probability is
easily calculated by using the following equation:

\begin{equation}
P(D_i \mid \Phi) = \frac{N_{D_i\&\Phi}}{N_{\Phi}}
\end{equation}

With $N_{D_i\&\Phi}$ being the number of occurrences of the value $D_i$ within
$\Phi$ and $N_{\Phi}$ being the number of occurrences of a certain value of
$\Phi$ with respect all the $\Phi$ values.

Therefore given a specific sub-dimension value $d_i=k$, the probability of a
pixel having that value given that it has been cataloged as background is given
by the following expression:

\begin{equation}
P(k \mid bg) = \frac{N_{k\&fg}}{N_{fg}}
\end{equation}

With $N_{k\&fg}$ being the number of times a foreground pixel took the value of
$k$ and $N_{fg}$ is the number of foreground pixels.

In general we model each discrete value of each sub-dimension in
each pixel. This allows us to answer questions like: What is the probability of
a pixel being foreground given the value of the pixel? Likewise for background.
These probabilities are important for our ultimate goal decide if a new pixel
is foreground or background\footnote{foreground being an element of interest
and background being other stuff}. By knowing which of these probabilities is
the greatest we are able to decide if a pixel is in the background group or the
foreground one. The following equation is true for pixels that are foreground
and false for the rest.

\begin{equation}\label{eq:comparison}
P(fg \mid d) >= P(bg \mid d)
\end{equation}

Notice that the denominator of both sides in equation \ref{eq:comparison}
cancels out. This means that there is no need to calculate $P(D)$ in the model.

\section{Continuous Model}

\listoffixmes
\end{document}

